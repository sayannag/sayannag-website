<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sayan Nag</title>
  
  <meta name="author" content="Sayan Nag">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <script src='https://kit.fontawesome.com/a076d05399.js' crossorigin='anonymous'></script>
  
  <style>
body {
  background-color: white;
  font-family: cursive;
}

.glow {
  font-size: 15px;
  color: red;
  text-align: center;
  animation: glow 1s ease-in-out infinite alternate;
}

@-webkit-keyframes glow {
  from {
    text-shadow: 0 0 2px #fff, 0 0 3px #fff, 0 0 4px #e60073, 0 0 5px #e60073, 0 0 6px #e60073, 0 0 7px #e60073, 0 0 8px #e60073;
  }
  
  to {
    text-shadow: 0 0 4px #fff, 0 0 5px #ff4da6, 0 0 6px #ff4da6, 0 0 7px #ff4da6, 0 0 8px #ff4da6, 0 0 9px #ff4da6, 0 0 10px #ff4da6;
  }
}
</style>
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="images/icon.jpg">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Sayan Nag (সায়ন নাগ)</name>
              </p>
              <p>I am a PhD student at the <a href="https://www.utoronto.ca/">University of Toronto</a>, where I work on Artificial Intelligence and Neuroscience. I have completed my undergraduate studies in Electrical Engineering from <a href="http://www.jaduniv.edu.in/">Jadavpur University</a>, India.
              </p>
              <p>
                Apart from research, I co-founded <a href="https://www.linkedin.com/company/drivefile">Drivefile</a> which is an accountability system for police brutality. I am the co-creator of a graduate school med-science blog called <a href="https://gradschooldigest.github.io/gradschooldigest/">GradSchoolDigest</a>.
			  </p>
              <p style="text-align:center">
                <a href="mailto:sayan.nag@mail.utoronto.ca">Email</a> &nbsp | &nbsp
                <!--<a href="data/SayanNag-CV.pdf">CV</a> &nbsp/&nbsp-->
                <a href="https://scholar.google.com/citations?user=K8w4dj4AAAAJ&hl=en">Google Scholar</a> &nbsp | &nbsp
                <a href="https://twitter.com/nagsayan112358?lang=en">Twitter</a> &nbsp | &nbsp
                <a href="https://github.com/sayannag">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Snag1.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Snag1circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Updates</heading>
			  <div style="width:100%;overflow-y:scroll; height:200px;">
              <ul>
			  <li> VistaLLM and MelFusion are selected as <span style='color:red'><strong>Highlights</strong></span> (<span style='color:red'><strong>Top 2.8%</strong></span> of submitted papers) at CVPR 2024! <strong><span class="glow">NEW</span></strong></li>
			  <li> VistaLLM and MelFusion are accepted at CVPR 2024! <strong><span class="glow">NEW</span></strong></li>
			  <li> ApoLLo has been accepted at EMNLP 2023!</li>
			  <li> VoLTA has been accepted at TMLR 2023!</li>
			  <li> EgoVLPv2 has been accepted at ICCV 2023!</li>
			  <li> Joined <a href="https://research.adobe.com/"><span style='color:red'><strong>Adobe Research</strong></span></a> as a research intern!</li>
			  <li> BeAts has been accepted at Interspeech 2023 as an <strong>Oral presentation</strong>!</li>
			  <li> DeCAtt has been accepted at CVPRw 2023 as an <strong>Oral presentation</strong>!</li>
			  <li> IDEAL has been accepted at ICASSP 2023!</li>
		      <li> Our paper on SERF: Towards better training of deep neural networks using log-Softplus ERror activation Function has been accepted at WACV 2023!</li>
              <li> Our paper on Deciphering Environmental Air Pollution with Large Scale City Data has been accepted at at IJCAI 2022 as an <strong>Oral Presentation</strong>!</li>
			  <li> Our abstract on Fast and scalable estimation of effective connectivity using Neural Network aided P-DCM has been accepted at the OHBM, 2022!</li>
			  <li> My paper on Graph Self Supervised Learning: the BT, the HSIC, and the VICReg has been presented at IJCAI Weakly Supervised Representation Learning Workshop 2021!</li>
			  <li> Our paper on CDF-Net: Cross-Domain Fusion Network for Accelerated MRI Reconstruction has been presented at MICCAI 2020!</li>
			</ul>
			</div>
            </td>
          </tr>
        </tbody>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research interests broadly include <b>Computer Vision</b>, <b>Self-supervised Learning</b>, <b>Multimodal Learning</b>, <b>Time-series Modeling</b> and <b>Natural Language Understanding</b>. I also work in ML for <b>Climate Change</b> <i class="fa fa-globe" style="color:blue"></i> and ML for <b>Health</b><i class='	fa fa-heartbeat' style='color:red'></i>. Previously, I have worked on Approximate Optimzation Algorithms and Image Processing. Some <strong>recent</strong> representative papers can be found below. Other publications can be found in my <a href="https://scholar.google.com/citations?user=K8w4dj4AAAAJ&hl=en">Google Scholar</a> link.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
			
		
		<tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/melfusion-diagram.jpg" alt="project image" width="150" height="80">
            </div>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>MeLFusion: Synthesizing Music from Image and Language Cues using Diffusion Models</papertitle>
              <br>
              Sanjoy Chowdhury*, <strong>Sayan Nag*</strong>, Joseph KJ, Balaji Vasan Srinivasan, Dinesh Manocha
              <br>
              <em>CVPR</em>, 2024 &nbsp <font color="red"><strong>(Highlight, Top 2.8%)</strong></font>
              <br>
			  <p> <a href="">Paper</a> | <a href="https://schowdhury671.github.io/melfusion_cvpr2024/">Project</a> | Code (Coming Soon)				
              <p></p>
            </td>
          </tr>
		  
		  <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/VistaLLM_Sampling.png" alt="VistaLLM_Sampling" width="150" height="70">
            </div>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model</papertitle>
              <br>
              Shraman Pramanick*, Guangxing Han*, Rui Hou, <strong>Sayan Nag</strong>, Ser-Nam Lim, Nicolas Ballas, Qifan Wang, Rama Chellappa, Amjad Almahairi
              <br>
              <em>CVPR</em>, 2024 &nbsp <font color="red"><strong>(Highlight, Top 2.8%)</strong></font>
              <br>
			  <p> <a href="https://arxiv.org/abs/2312.12423">Paper</a> | <a href="https://shramanpramanick.github.io/VistaLLM/">Project</a> | Code (Coming Soon)				
              <p></p>
            </td>
          </tr>
		  
		  <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/apollo.jpg" alt="VistaLLM_Sampling" width="150" height="70">
            </div>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>APoLLo: Unified Adapter and Prompt Learning for Vision Language Models</papertitle>
              <br>
              Sanjoy Chowdhury*, <strong>Sayan Nag*</strong>, Dinesh Manocha
              <br>
              <em>EMNLP</em>, 2023
              <br>
			  <p> <a href="https://aclanthology.org/2023.emnlp-main.629.pdf">Paper</a> | <a href="https://github.com/schowdhury671/APoLLo">Code</a>	| <a href="https://gamma.umd.edu/pro/vision_language/apollo/">Project</a> 			
              <p></p>
            </td>
          </tr>
		 
		  <tr>
            <td style="padding:15px;width:25%;vertical-align:middle">
              <divI style="text-align: center">
              <img src="images/radar_egovlpv2-transparent.png" alt="EgoVLPv2" width="180" height="165">
            </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                 <papertitle>EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the Backbone</papertitle>         
            <br>
            Shraman Pramanick, Yale Song, <strong>Sayan Nag</strong>, Kevin Qinghong Lin, Hardik Shah, Mike Z. Shou, Rama Chellappa, Pengchuan Zhang
            <br>
            <em>ICCV, 2023</em>

        <p> <a href="https://arxiv.org/pdf/2307.05463.pdf">Paper</a> | <a href="https://github.com/facebookresearch/EgoVLPv2">Code</a> | <a href="https://shramanpramanick.github.io/EgoVLPv2/">Project</a>
            </td>
          </tr>
		  
		  <tr>
            <td style="padding:15px;width:25%;vertical-align:middle;padding-right:20px">
              <div style="text-align: center">
              <img src="images/VoLTA_Alignment.png" alt="VoLTA" width="150" height="115">
            </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                 <papertitle>VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment</papertitle>           
            <br>
            Shraman Pramanick*, Li Jing*, <strong>Sayan Nag*</strong>, Jiachen Zhu, Hardik Shah, Yann LeCun, Rama Chellappa
            <br>
            <em>TMLR, 2023</em>

        <p> <a href="https://arxiv.org/pdf/2210.04135.pdf">Paper</a> | <a href="https://github.com/ShramanPramanick/VoLTA">Code</a> | <a href="https://shramanpramanick.github.io/VoLTA/">Project</a> </p>
            </td>
          </tr>
		  
		  <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/interspeech23.png" alt="VistaLLM_Sampling" width="130" height="100">
            </div>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>BeAts: Bengali Speech Acts Recognition using Multimodal Attention Fusion</papertitle>
              <br>
              Ahana Deb*, <strong>Sayan Nag*</strong>, Ayan Mahapatra*, Soumitri Chattopadhyay*, Aritra Marik*, Pijush Kanti Gayen, Shankha Sanyal, Archi Banerjee, Samir Karmakar
              <br>
              <em>Interspeech</em>, 2023 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
			  <p> <a href="https://arxiv.org/pdf/2306.02680.pdf">Paper</a> | <a href="https://soumitri2001.github.io/BeAts">Project</a> 			
              <p></p>
            </td>
          </tr>
		  
		  <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/icassp23.png" alt="VistaLLM_Sampling" width="150" height="100">
            </div>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>IDEAL: Improved DEnse locAL Contrastive Learning for Semi-Supervised Medical Image Segmentation</papertitle>
              <br>
              Hritam Basak, Soumitri Chattopadhyay*, Rohit Kundu*, <strong>Sayan Nag*</strong>, Rammohan Mallipeddi
              <br>
              <em>ICASSP</em>, 2023
              <br>
			  <p> <a href="https://arxiv.org/pdf/2210.15075.pdf">Paper</a> | <a href="https://github.com/Rohit-Kundu/IDEAL-ICASSP23">Code</a> | <a href="https://rohit-kundu.github.io/IDEAL-ICASSP23/">Project</a> 			
              <p></p>
            </td>
          </tr>
		  
		  <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/cvprw23.jpg" alt="VistaLLM_Sampling" width="150" height="70">
            </div>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>DeCAtt: Efficient Vision Transformers with Decorrelated Attention Heads</papertitle>
              <br>
              Mayukh Bhattacharyya*, Soumitri Chattopadhyay*, <strong>Sayan Nag*</strong>
              <br>
              <em>CVPRW</em>, 2023 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
			  <p><a href="https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Bhattacharyya_DeCAtt_Efficient_Vision_Transformers_With_Decorrelated_Attention_Heads_CVPRW_2023_paper.pdf">Paper</a>			
              <p></p>
            </td>
          </tr>
		  
		  <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:15px;width:25%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/icip23.png" alt="VistaLLM_Sampling" width="150" height="120">
            </div>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Exploring Self-Supervised Representation Learning For Low-Resource Medical Image Analysis</papertitle>
              <br>
              Soumitri Chattopadhyay, Soham Ganguly*, Sreejit Chaudhury*, <strong>Sayan Nag*</strong>, Samiran Chattopadhyay
              <br>
              <em>ICIP</em>, 2023
              <br>
			  <p> <a href="https://arxiv.org/pdf/2303.02245.pdf">Paper</a> | <a href="https://github.com/soumitri2001/SmallDataSSL">Code</a>			
              <p></p>
            </td>
          </tr>
					
          <tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
            <td style="padding:15px;width:25%;vertical-align:middle;padding-right:20px">
              <div class="one">
                <img src='images/Airpoll3.png' height="130", width="150">
              </div>
              <script type="text/javascript">
                function mip360_start() {
                  document.getElementById('mip360_image').style.opacity = "1";
                }

                function mip360_stop() {
                  document.getElementById('mip360_image').style.opacity = "0";
                }
                mip360_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Deciphering Environmental Air Pollution with Large Scale City Data</papertitle>
              <br>
			  Mayukh Bhattacharyya*,
              <strong>Sayan Nag*</strong>,
			  Udita Ghosh
              <br>
							<em>IJCAI</em>, 2022 &nbsp <font color="red"><strong>(Spotlight & Oral Presentation)</strong></font>
              <br>
			  <p>* denotes equal contribution</p>
				<a href="https://www.ijcai.org/proceedings/2022/0698.pdf">Paper</a> | <a href="https://mayukh18.github.io/DEAP/">Project</a> | <a href="https://github.com/mayukh18/DEAP">Code</a>
              <p></p>
              <p>Air pollutant forecasting using a novel <strong><span style="font-family:Courier">cosSquareFormer</span></strong>.</p>


</p>
            </td>
          </tr> 
		  
	<tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:15px;width:25%;vertical-align:middle;padding-right:20px">
              <div class="one">
                <center><img src='images/Serf.png' height="150", width="150"></center>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>SERF: Towards better training of deep neural networks using log-Softplus ERror activation Function</papertitle>
              <br>
              <strong>Sayan Nag*</strong>,  Mayukh Bhattacharyya*, Anuraag Mukherjee*, Rohit Kundu*
              <br>
              <em>WACV</em>, 2023 
              <br>
			  <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Nag_SERF_Towards_Better_Training_of_Deep_Neural_Networks_Using_Log-Softplus_WACV_2023_paper.pdf">Paper</a>				
              <p></p>
            </td>
          </tr>

    <tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
            <td style="padding:15px;width:25%;vertical-align:middle;padding-right:20px">
              <div class="one">
                <img src='images/GraphVICRegHSIC.png' height="150", width="150">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Graph Self Supervised Learning: the BT, the HSIC, and the VICReg</papertitle>
              <br>
              <strong>Sayan Nag</strong>
              <br>
							<em>IJCAI Weakly Supervised Representation Learning Workshop (IJCAI-WSRL)</em>, 2021
              <br>
			  <a href="https://arxiv.org/pdf/2105.12247.pdf">Paper</a> | 
			  <a href="data/IJCAI_WSRL_poster_paper_ID_20_Sayan_Nag.pdf">Poster</a>
              <p></p>
              <p>Self-Supervised Learning using Graph Neural Networks and a hybrid <strong>VICRegHSIC</strong> loss function.</p>


</p>
            </td>
          </tr> 
		  
		<tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
            <td style="padding:15px;width:25%;vertical-align:middle;padding-right:20px">
              <div class="one">
                <img src='images/CDFNet.png' height="150", width="150">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>CDF-Net: Cross-Domain Fusion Network for Accelerated MRI Reconstruction</papertitle>
              <br>
			  Osvald Nitski, 
              <strong>Sayan Nag</strong>, Chris McIntosh, Bo Wang
              <br>
							<em>MICCAI</em>, 2020
              <br>
			  <a href="https://link.springer.com/chapter/10.1007/978-3-030-59713-9_41">Paper</a>
              <p></p>


</p>
            </td>
          </tr>
		  
		 <tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
            <td style="padding:15px;width:25%;vertical-align:middle;padding-right:20px">
              <div class="one">
                <img src='images/HybridSiamese.png' height="150", width="150">
              </div>
              <script type="text/javascript">
                function mip360_start() {
                  document.getElementById('mip360_image').style.opacity = "1";
                }

                function mip360_stop() {
                  document.getElementById('mip360_image').style.opacity = "0";
                }
                mip360_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Hybrid Style Siamese Network: Incorporating style loss in complementary apparels retrieval</papertitle>
              <br>
			  Mayukh Bhattacharyya,
              <strong>Sayan Nag</strong>,
              <br>
							<em>CVPR Workshop on Computer Vision for Fashion, Art and Design</em>, 2020
              <br>
			  <a href="https://arxiv.org/pdf/1912.05014.pdf">Paper</a> |
			  <a href="https://github.com/mayukh18/Hybrid-Style-Siamese-Network">Code</a> |
			  <a href="https://youtu.be/AyBzlWVhkRw">Video</a>
              <p></p>

</p>
            </td>
          </tr> 
		  
		 <tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
            <td style="padding:15px;width:25%;vertical-align:middle;padding-right:20px">
              <div class="one">
                <img src='images/DeepMusic.png' height="150", width="150">
              </div>
              <script type="text/javascript">
                function mip360_start() {
                  document.getElementById('mip360_image').style.opacity = "1";
                }

                function mip360_stop() {
                  document.getElementById('mip360_image').style.opacity = "0";
                }
                mip360_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>On the application of deep learning and multifractal techniques to classify emotions and instruments using Indian Classical Music</papertitle>
              <br>
              <strong>Sayan Nag</strong>, Medha Basu, Shankha Sanyal, Archi Banerjee, Dipak Ghosh
              <br>
							<em>Physica A: Statistical Mechanics and its Applications</em>, 2022
              <br>
			  <a href="https://www.sciencedirect.com/science/article/abs/pii/S0378437122002291">Paper</a>
              <p></p>
              <p>A new dataset comprising of <strong>Indian Classical Music</strong> clips is proposed along with a Neural ODE based architecture for MER and MIR tasks.</p>


</p>
            </td>
          </tr> 

        </tbody></table>
		
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Some Fun Projects</heading>
              <p>
			  This section consists of some fun projects that I have undertaken during my leisure times.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
					
          <tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/DeOldify.png' height="130", width="150">
              </div>
              <script type="text/javascript">
                function mip360_start() {
                  document.getElementById('mip360_image').style.opacity = "1";
                }

                function mip360_stop() {
                  document.getElementById('mip360_image').style.opacity = "0";
                }
                mip360_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Colorization of Old Movies using Deep Learning</papertitle>
              <br>
              <strong>Sayan Nag</strong>
			  <br>
			  <a href="https://youtu.be/HhV-WBj2tQQ">video</a>
              <p></p>
              <p>This video is a short clip from the movie <a href="https://www.imdb.com/title/tt0060742/">Nayak</a> which shows the famous money scene.
			  This work has been done by using <a href="https://arxiv.org/pdf/1805.08318.pdf">self-attention GAN (SAGAN)</a> which colors and restores old images and videos.
			  This movie is considered as one of the most iconic films in the history of Bengali Cinema. It is also one of my grandmom's favorites.</p>


</p>
            </td>
          </tr> 
		  
		<tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/StyleTransferPaint.png' height="130", width="150">
              </div>
              <script type="text/javascript">
                function mip360_start() {
                  document.getElementById('mip360_image').style.opacity = "1";
                }

                function mip360_stop() {
                  document.getElementById('mip360_image').style.opacity = "0";
                }
                mip360_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Neural Style Transfer with My Paintings</papertitle>
              <br>
              <strong>Sayan Nag</strong>
			  <br>
              <p></p>
              <p>In this small fun project, I have used my paintings to do <a href="https://en.wikipedia.org/wiki/Neural_style_transfer">Neural Style Transfer</a>.</p>


</p>
            </td>
          </tr>
		  

        </tbody></table>
		
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
                Template borrowed from <a href="https://jonbarron.info/">Jon Barron</a>'s website.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
